{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29239fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aadit\\OneDrive\\Desktop\\data_science\\projects\\sleepquality\\music_sleep_data\\Data\n"
     ]
    }
   ],
   "source": [
    "%cd C:\\Users\\aadit\\OneDrive\\Desktop\\data_science\\projects\\sleepquality\\music_sleep_data\\Data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ec02d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "585a4b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50f5eab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"musslp_cleaneddata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4dbcb9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participantID</th>\n",
       "      <th>demAGE</th>\n",
       "      <th>demFEMALE</th>\n",
       "      <th>demINCOME</th>\n",
       "      <th>demLADDER</th>\n",
       "      <th>demNONCHINESE</th>\n",
       "      <th>chrMED</th>\n",
       "      <th>chrCAF</th>\n",
       "      <th>musicYearsLessons</th>\n",
       "      <th>musicYearsPractice</th>\n",
       "      <th>conditionH</th>\n",
       "      <th>conditionS</th>\n",
       "      <th>conditionP</th>\n",
       "      <th>conditionMusic</th>\n",
       "      <th>day</th>\n",
       "      <th>dailySleep</th>\n",
       "      <th>dailyStress</th>\n",
       "      <th>dailyNAO</th>\n",
       "      <th>dailyPAO</th>\n",
       "      <th>dailyLS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>874.000000</td>\n",
       "      <td>874.000000</td>\n",
       "      <td>874.000000</td>\n",
       "      <td>874.000000</td>\n",
       "      <td>874.000000</td>\n",
       "      <td>874.000000</td>\n",
       "      <td>874.000000</td>\n",
       "      <td>874.000000</td>\n",
       "      <td>874.000000</td>\n",
       "      <td>874.000000</td>\n",
       "      <td>874.000000</td>\n",
       "      <td>874.000000</td>\n",
       "      <td>874.000000</td>\n",
       "      <td>874.000000</td>\n",
       "      <td>874.000000</td>\n",
       "      <td>874.000000</td>\n",
       "      <td>874.000000</td>\n",
       "      <td>874.000000</td>\n",
       "      <td>874.000000</td>\n",
       "      <td>874.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>31.426773</td>\n",
       "      <td>21.839817</td>\n",
       "      <td>0.764302</td>\n",
       "      <td>4.244851</td>\n",
       "      <td>5.943936</td>\n",
       "      <td>0.207094</td>\n",
       "      <td>0.017162</td>\n",
       "      <td>0.593822</td>\n",
       "      <td>3.975973</td>\n",
       "      <td>3.453089</td>\n",
       "      <td>0.328375</td>\n",
       "      <td>0.338673</td>\n",
       "      <td>0.332952</td>\n",
       "      <td>0.667048</td>\n",
       "      <td>3.002288</td>\n",
       "      <td>3.810069</td>\n",
       "      <td>4.725400</td>\n",
       "      <td>1.500636</td>\n",
       "      <td>2.308035</td>\n",
       "      <td>3.356979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>17.826766</td>\n",
       "      <td>2.301345</td>\n",
       "      <td>0.424677</td>\n",
       "      <td>2.400268</td>\n",
       "      <td>1.170589</td>\n",
       "      <td>0.405455</td>\n",
       "      <td>0.129951</td>\n",
       "      <td>0.491400</td>\n",
       "      <td>4.472199</td>\n",
       "      <td>4.465032</td>\n",
       "      <td>0.469891</td>\n",
       "      <td>0.473529</td>\n",
       "      <td>0.471539</td>\n",
       "      <td>0.471539</td>\n",
       "      <td>1.406089</td>\n",
       "      <td>3.367262</td>\n",
       "      <td>2.797089</td>\n",
       "      <td>0.660170</td>\n",
       "      <td>0.914296</td>\n",
       "      <td>0.819733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.555556</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>32.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.222222</td>\n",
       "      <td>2.111111</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>47.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>62.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       participantID      demAGE   demFEMALE   demINCOME   demLADDER  \\\n",
       "count     874.000000  874.000000  874.000000  874.000000  874.000000   \n",
       "mean       31.426773   21.839817    0.764302    4.244851    5.943936   \n",
       "std        17.826766    2.301345    0.424677    2.400268    1.170589   \n",
       "min         1.000000   19.000000    0.000000    1.000000    3.000000   \n",
       "25%        16.000000   20.000000    1.000000    2.000000    5.000000   \n",
       "50%        32.000000   21.000000    1.000000    4.000000    6.000000   \n",
       "75%        47.000000   23.000000    1.000000    6.000000    7.000000   \n",
       "max        62.000000   31.000000    1.000000    9.000000    9.000000   \n",
       "\n",
       "       demNONCHINESE      chrMED      chrCAF  musicYearsLessons  \\\n",
       "count     874.000000  874.000000  874.000000         874.000000   \n",
       "mean        0.207094    0.017162    0.593822           3.975973   \n",
       "std         0.405455    0.129951    0.491400           4.472199   \n",
       "min         0.000000    0.000000    0.000000           0.000000   \n",
       "25%         0.000000    0.000000    0.000000           0.000000   \n",
       "50%         0.000000    0.000000    1.000000           2.000000   \n",
       "75%         0.000000    0.000000    1.000000           7.000000   \n",
       "max         1.000000    1.000000    1.000000          16.000000   \n",
       "\n",
       "       musicYearsPractice  conditionH  conditionS  conditionP  conditionMusic  \\\n",
       "count          874.000000  874.000000  874.000000  874.000000      874.000000   \n",
       "mean             3.453089    0.328375    0.338673    0.332952        0.667048   \n",
       "std              4.465032    0.469891    0.473529    0.471539        0.471539   \n",
       "min              0.000000    0.000000    0.000000    0.000000        0.000000   \n",
       "25%              0.000000    0.000000    0.000000    0.000000        0.000000   \n",
       "50%              0.000000    0.000000    0.000000    0.000000        1.000000   \n",
       "75%              7.000000    1.000000    1.000000    1.000000        1.000000   \n",
       "max             14.000000    1.000000    1.000000    1.000000        1.000000   \n",
       "\n",
       "              day  dailySleep  dailyStress    dailyNAO    dailyPAO     dailyLS  \n",
       "count  874.000000  874.000000   874.000000  874.000000  874.000000  874.000000  \n",
       "mean     3.002288    3.810069     4.725400    1.500636    2.308035    3.356979  \n",
       "std      1.406089    3.367262     2.797089    0.660170    0.914296    0.819733  \n",
       "min      1.000000    0.000000     0.000000    1.000000    1.000000    1.000000  \n",
       "25%      2.000000    1.000000     3.000000    1.000000    1.555556    3.000000  \n",
       "50%      3.000000    3.000000     5.000000    1.222222    2.111111    3.000000  \n",
       "75%      4.000000    6.000000     7.000000    1.666667    3.000000    4.000000  \n",
       "max      5.000000   14.000000    10.000000    4.666667    5.000000    5.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "624de0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['participantID', 'dailySleep'], axis=1)\n",
    "y = data['dailySleep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ae1d0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_models=7\n",
    "models=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba5d6724",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = ['condition','stimulusID',]\n",
    "X_encoded = pd.get_dummies(X, columns=categorical_columns, prefix=categorical_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe6160a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29b59fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "18/18 [==============================] - 1s 11ms/step - loss: 29.6824 - mae: 4.1319 - val_loss: 27.5822 - val_mae: 3.9519\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 25.6157 - mae: 3.8876 - val_loss: 26.3229 - val_mae: 3.8301\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.5675 - mae: 3.6640 - val_loss: 25.5928 - val_mae: 3.7703\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.3264 - mae: 3.6732 - val_loss: 25.7023 - val_mae: 3.7796\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.2600 - mae: 3.5755 - val_loss: 25.4537 - val_mae: 3.7604\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 21.5372 - mae: 3.5398 - val_loss: 24.9136 - val_mae: 3.7141\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.9548 - mae: 3.4487 - val_loss: 24.6843 - val_mae: 3.6957\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.8922 - mae: 3.4320 - val_loss: 23.9891 - val_mae: 3.6366\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.6251 - mae: 3.3949 - val_loss: 23.3774 - val_mae: 3.5823\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 21.5041 - mae: 3.5031 - val_loss: 23.4718 - val_mae: 3.5921\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.9587 - mae: 3.4100 - val_loss: 23.2087 - val_mae: 3.5702\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.9225 - mae: 3.5218 - val_loss: 22.8677 - val_mae: 3.5410\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.6471 - mae: 3.3760 - val_loss: 22.5283 - val_mae: 3.5095\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.1999 - mae: 3.4372 - val_loss: 22.2423 - val_mae: 3.4829\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.5365 - mae: 3.3223 - val_loss: 22.2893 - val_mae: 3.4868\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.9799 - mae: 3.3816 - val_loss: 22.4182 - val_mae: 3.4970\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.3764 - mae: 3.3288 - val_loss: 22.3687 - val_mae: 3.4873\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.4005 - mae: 3.3711 - val_loss: 22.3994 - val_mae: 3.4923\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.5593 - mae: 3.2725 - val_loss: 22.2763 - val_mae: 3.4802\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.6623 - mae: 3.3798 - val_loss: 21.8094 - val_mae: 3.4333\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.9188 - mae: 3.3825 - val_loss: 22.1488 - val_mae: 3.4645\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.7389 - mae: 3.3773 - val_loss: 22.2544 - val_mae: 3.4758\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.9381 - mae: 3.2755 - val_loss: 21.9312 - val_mae: 3.4429\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.3644 - mae: 3.2542 - val_loss: 21.8986 - val_mae: 3.4406\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.5429 - mae: 3.2581 - val_loss: 21.7238 - val_mae: 3.4231\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.0934 - mae: 3.2507 - val_loss: 21.7008 - val_mae: 3.4187\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.0302 - mae: 3.3366 - val_loss: 21.5229 - val_mae: 3.4003\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.0628 - mae: 3.3835 - val_loss: 21.4420 - val_mae: 3.3933\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.3422 - mae: 3.3714 - val_loss: 21.5147 - val_mae: 3.4004\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.8619 - mae: 3.5402 - val_loss: 21.9922 - val_mae: 3.4497\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.5600 - mae: 3.2321 - val_loss: 21.5691 - val_mae: 3.4077\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.5151 - mae: 3.3498 - val_loss: 21.2843 - val_mae: 3.3787\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.3098 - mae: 3.3364 - val_loss: 21.3095 - val_mae: 3.3825\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.3826 - mae: 3.3240 - val_loss: 21.1169 - val_mae: 3.3637\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.0336 - mae: 3.2367 - val_loss: 20.8332 - val_mae: 3.3369\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.6318 - mae: 3.2533 - val_loss: 20.7722 - val_mae: 3.3319\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.5346 - mae: 3.3554 - val_loss: 20.7445 - val_mae: 3.3302\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.4166 - mae: 3.3410 - val_loss: 20.5708 - val_mae: 3.3110\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.8489 - mae: 3.2451 - val_loss: 20.4659 - val_mae: 3.2990\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.2550 - mae: 3.1979 - val_loss: 20.5605 - val_mae: 3.3080\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.1399 - mae: 3.3332 - val_loss: 20.6573 - val_mae: 3.3172\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.9626 - mae: 3.3559 - val_loss: 20.8312 - val_mae: 3.3335\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.5006 - mae: 3.2603 - val_loss: 20.5351 - val_mae: 3.3039\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.7416 - mae: 3.2994 - val_loss: 20.4373 - val_mae: 3.2932\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.8954 - mae: 3.3939 - val_loss: 20.1640 - val_mae: 3.2661\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.8597 - mae: 3.2993 - val_loss: 20.2813 - val_mae: 3.2777\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 17.5259 - mae: 3.1949 - val_loss: 20.1019 - val_mae: 3.2607\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.1094 - mae: 3.3629 - val_loss: 19.8657 - val_mae: 3.2393\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.9791 - mae: 3.3486 - val_loss: 20.2065 - val_mae: 3.2703\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.5879 - mae: 3.3696 - val_loss: 20.3618 - val_mae: 3.2860\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 1s 9ms/step - loss: 64.1145 - mae: 5.7213 - val_loss: 17.0915 - val_mae: 3.1700\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 32.9629 - mae: 4.3696 - val_loss: 20.1181 - val_mae: 3.3947\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 26.4733 - mae: 3.9710 - val_loss: 21.7493 - val_mae: 3.5429\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 25.2257 - mae: 3.8862 - val_loss: 21.9153 - val_mae: 3.5594\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.6338 - mae: 3.7163 - val_loss: 21.5012 - val_mae: 3.5208\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.5381 - mae: 3.7262 - val_loss: 21.2382 - val_mae: 3.4961\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.3605 - mae: 3.6489 - val_loss: 21.3306 - val_mae: 3.5047\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 21.4464 - mae: 3.5005 - val_loss: 21.2510 - val_mae: 3.4961\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.1844 - mae: 3.5770 - val_loss: 21.7047 - val_mae: 3.5387\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 21.8702 - mae: 3.5665 - val_loss: 21.9248 - val_mae: 3.5602\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.4305 - mae: 3.3800 - val_loss: 21.8209 - val_mae: 3.5503\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.6402 - mae: 3.4099 - val_loss: 21.3809 - val_mae: 3.5089\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 21.3921 - mae: 3.4942 - val_loss: 21.1047 - val_mae: 3.4835\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.4026 - mae: 3.4529 - val_loss: 21.3750 - val_mae: 3.5104\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 21.0096 - mae: 3.5159 - val_loss: 21.7639 - val_mae: 3.5470\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 21.3013 - mae: 3.4921 - val_loss: 21.5553 - val_mae: 3.5279\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 21.2383 - mae: 3.4450 - val_loss: 21.6286 - val_mae: 3.5358\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.2007 - mae: 3.3503 - val_loss: 21.4015 - val_mae: 3.5145\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 21.3816 - mae: 3.4733 - val_loss: 21.2944 - val_mae: 3.5042\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 20.0839 - mae: 3.3868 - val_loss: 21.2259 - val_mae: 3.4983\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.5702 - mae: 3.3327 - val_loss: 21.1892 - val_mae: 3.4961\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.7181 - mae: 3.4253 - val_loss: 20.9241 - val_mae: 3.4716\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.1408 - mae: 3.3964 - val_loss: 20.9437 - val_mae: 3.4738\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.7324 - mae: 3.4474 - val_loss: 21.0015 - val_mae: 3.4773\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.0584 - mae: 3.4042 - val_loss: 20.8913 - val_mae: 3.4672\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.5785 - mae: 3.3943 - val_loss: 20.8796 - val_mae: 3.4658\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.8962 - mae: 3.3728 - val_loss: 20.9492 - val_mae: 3.4724\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.2732 - mae: 3.3274 - val_loss: 20.5245 - val_mae: 3.4328\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.5750 - mae: 3.3060 - val_loss: 20.1218 - val_mae: 3.3937\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.6056 - mae: 3.4317 - val_loss: 20.6675 - val_mae: 3.4452\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 21.1982 - mae: 3.4732 - val_loss: 21.0222 - val_mae: 3.4786\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.7363 - mae: 3.3436 - val_loss: 20.5240 - val_mae: 3.4309\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.8858 - mae: 3.2774 - val_loss: 19.7574 - val_mae: 3.3570\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.3115 - mae: 3.3386 - val_loss: 19.9055 - val_mae: 3.3721\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.7989 - mae: 3.3865 - val_loss: 19.5461 - val_mae: 3.3406\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.8789 - mae: 3.4220 - val_loss: 19.9841 - val_mae: 3.3810\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.5627 - mae: 3.3057 - val_loss: 19.9982 - val_mae: 3.3836\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.4825 - mae: 3.3449 - val_loss: 19.9434 - val_mae: 3.3788\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.2642 - mae: 3.3295 - val_loss: 19.6907 - val_mae: 3.3571\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.8441 - mae: 3.3395 - val_loss: 19.6283 - val_mae: 3.3531\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.6297 - mae: 3.3691 - val_loss: 19.7583 - val_mae: 3.3656\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.2581 - mae: 3.2889 - val_loss: 19.8282 - val_mae: 3.3737\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.0705 - mae: 3.3056 - val_loss: 19.8161 - val_mae: 3.3713\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.3416 - mae: 3.3581 - val_loss: 19.7785 - val_mae: 3.3650\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.6947 - mae: 3.3854 - val_loss: 20.0332 - val_mae: 3.3832\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.9920 - mae: 3.2681 - val_loss: 19.4217 - val_mae: 3.3277\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.1922 - mae: 3.3149 - val_loss: 18.8586 - val_mae: 3.2835\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.4327 - mae: 3.3259 - val_loss: 18.9834 - val_mae: 3.2936\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.2686 - mae: 3.3347 - val_loss: 18.7976 - val_mae: 3.2807\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.0177 - mae: 3.3073 - val_loss: 18.7637 - val_mae: 3.2765\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 1s 9ms/step - loss: 35.4068 - mae: 4.4467 - val_loss: 26.6488 - val_mae: 3.9943\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.2769 - mae: 3.6837 - val_loss: 29.9331 - val_mae: 4.2763\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.2836 - mae: 3.6859 - val_loss: 30.5162 - val_mae: 4.3258\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 21.0216 - mae: 3.4382 - val_loss: 30.7674 - val_mae: 4.3472\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 21.2838 - mae: 3.4831 - val_loss: 31.4749 - val_mae: 4.4075\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.4916 - mae: 3.3095 - val_loss: 31.2781 - val_mae: 4.3915\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.3521 - mae: 3.3427 - val_loss: 30.6647 - val_mae: 4.3431\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.5250 - mae: 3.2930 - val_loss: 30.4004 - val_mae: 4.3227\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.0468 - mae: 3.4156 - val_loss: 30.1869 - val_mae: 4.3053\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.3490 - mae: 3.3816 - val_loss: 30.6895 - val_mae: 4.3457\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.8590 - mae: 3.2821 - val_loss: 30.9728 - val_mae: 4.3691\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.5203 - mae: 3.3634 - val_loss: 30.8528 - val_mae: 4.3604\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.5375 - mae: 3.3103 - val_loss: 30.2675 - val_mae: 4.3142\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.3670 - mae: 3.4531 - val_loss: 30.1617 - val_mae: 4.3048\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.4597 - mae: 3.2372 - val_loss: 30.1024 - val_mae: 4.3012\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.6684 - mae: 3.2245 - val_loss: 29.4495 - val_mae: 4.2482\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.4601 - mae: 3.1692 - val_loss: 29.1138 - val_mae: 4.2191\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.1077 - mae: 3.1834 - val_loss: 28.6142 - val_mae: 4.1779\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.8636 - mae: 3.2756 - val_loss: 29.0913 - val_mae: 4.2174\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.5302 - mae: 3.2107 - val_loss: 28.9381 - val_mae: 4.2049\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.5104 - mae: 3.2735 - val_loss: 29.2358 - val_mae: 4.2300\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.9314 - mae: 3.3835 - val_loss: 29.4650 - val_mae: 4.2477\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.8930 - mae: 3.2868 - val_loss: 29.4157 - val_mae: 4.2434\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.6407 - mae: 3.3590 - val_loss: 28.8258 - val_mae: 4.1961\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.2177 - mae: 3.1806 - val_loss: 28.6301 - val_mae: 4.1829\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 17.1044 - mae: 3.1262 - val_loss: 28.4131 - val_mae: 4.1672\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.2949 - mae: 3.2783 - val_loss: 28.7272 - val_mae: 4.1927\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 17.7328 - mae: 3.1577 - val_loss: 28.5043 - val_mae: 4.1723\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 17.9034 - mae: 3.1332 - val_loss: 27.5224 - val_mae: 4.0874\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 17.8411 - mae: 3.1824 - val_loss: 27.6219 - val_mae: 4.0966\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 17.1140 - mae: 3.0399 - val_loss: 27.2066 - val_mae: 4.0616\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.8408 - mae: 3.3122 - val_loss: 27.6065 - val_mae: 4.0968\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.0522 - mae: 3.1863 - val_loss: 27.5394 - val_mae: 4.0919\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.9937 - mae: 3.2668 - val_loss: 27.0510 - val_mae: 4.0499\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 17.8729 - mae: 3.2020 - val_loss: 27.1456 - val_mae: 4.0586\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 17.5740 - mae: 3.1162 - val_loss: 26.8510 - val_mae: 4.0332\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.0847 - mae: 3.1404 - val_loss: 26.7883 - val_mae: 4.0246\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 17.6968 - mae: 3.1270 - val_loss: 27.0245 - val_mae: 4.0456\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.6940 - mae: 3.2532 - val_loss: 26.7307 - val_mae: 4.0188\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 17.5855 - mae: 3.1524 - val_loss: 26.5969 - val_mae: 4.0062\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 17.1038 - mae: 3.1008 - val_loss: 26.2460 - val_mae: 3.9752\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.1497 - mae: 3.1601 - val_loss: 26.3690 - val_mae: 3.9851\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 17.4560 - mae: 3.1447 - val_loss: 26.3772 - val_mae: 3.9857\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.6981 - mae: 3.3701 - val_loss: 26.5115 - val_mae: 3.9980\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 17.6562 - mae: 3.1545 - val_loss: 26.6825 - val_mae: 4.0132\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 17.8793 - mae: 3.1448 - val_loss: 26.1824 - val_mae: 3.9684\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 17.8711 - mae: 3.2023 - val_loss: 25.9933 - val_mae: 3.9518\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.6151 - mae: 3.2146 - val_loss: 26.1460 - val_mae: 3.9657\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.8477 - mae: 3.2105 - val_loss: 26.2515 - val_mae: 3.9737\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.3196 - mae: 3.2518 - val_loss: 26.4739 - val_mae: 3.9947\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 1s 9ms/step - loss: 77.8449 - mae: 6.0168 - val_loss: 16.0206 - val_mae: 2.9496\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 37.7123 - mae: 4.6748 - val_loss: 21.8598 - val_mae: 3.4147\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 28.4172 - mae: 4.0440 - val_loss: 22.9523 - val_mae: 3.5147\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 29.3588 - mae: 4.2023 - val_loss: 23.3055 - val_mae: 3.5508\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 26.6480 - mae: 3.9875 - val_loss: 23.4768 - val_mae: 3.5689\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.5631 - mae: 3.7316 - val_loss: 23.5075 - val_mae: 3.5771\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.3451 - mae: 3.5783 - val_loss: 23.1260 - val_mae: 3.5446\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.5219 - mae: 3.6620 - val_loss: 22.7418 - val_mae: 3.5157\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 22.9834 - mae: 3.6968 - val_loss: 22.7257 - val_mae: 3.5157\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 21.7105 - mae: 3.4862 - val_loss: 22.4807 - val_mae: 3.4970\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.7703 - mae: 3.6545 - val_loss: 22.5582 - val_mae: 3.5032\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.7580 - mae: 3.6208 - val_loss: 22.3887 - val_mae: 3.4890\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.5143 - mae: 3.6237 - val_loss: 22.3447 - val_mae: 3.4856\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.3131 - mae: 3.6489 - val_loss: 22.4163 - val_mae: 3.4919\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.5749 - mae: 3.6710 - val_loss: 22.6321 - val_mae: 3.5103\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 21.8577 - mae: 3.5814 - val_loss: 22.3086 - val_mae: 3.4841\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.9737 - mae: 3.5192 - val_loss: 21.8975 - val_mae: 3.4498\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 21.4155 - mae: 3.5574 - val_loss: 21.6547 - val_mae: 3.4295\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.0679 - mae: 3.3390 - val_loss: 21.5683 - val_mae: 3.4223\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.5513 - mae: 3.4972 - val_loss: 21.5964 - val_mae: 3.4242\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 21.6718 - mae: 3.5272 - val_loss: 21.2293 - val_mae: 3.3931\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 21.8640 - mae: 3.5852 - val_loss: 21.6434 - val_mae: 3.4289\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.7198 - mae: 3.3603 - val_loss: 21.3872 - val_mae: 3.4077\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.6863 - mae: 3.3660 - val_loss: 21.0609 - val_mae: 3.3792\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.1034 - mae: 3.4387 - val_loss: 20.8232 - val_mae: 3.3586\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 21.1495 - mae: 3.5877 - val_loss: 21.0403 - val_mae: 3.3771\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.3832 - mae: 3.4271 - val_loss: 20.9172 - val_mae: 3.3663\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.9652 - mae: 3.3542 - val_loss: 20.8627 - val_mae: 3.3631\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.5264 - mae: 3.3338 - val_loss: 20.7902 - val_mae: 3.3589\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.1423 - mae: 3.4144 - val_loss: 20.6349 - val_mae: 3.3452\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.2472 - mae: 3.4610 - val_loss: 20.5837 - val_mae: 3.3404\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.6463 - mae: 3.3293 - val_loss: 20.3938 - val_mae: 3.3238\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.9065 - mae: 3.3294 - val_loss: 20.1301 - val_mae: 3.2999\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.5391 - mae: 3.3468 - val_loss: 19.8881 - val_mae: 3.2782\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.3962 - mae: 3.4728 - val_loss: 20.3988 - val_mae: 3.3244\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.8003 - mae: 3.5039 - val_loss: 20.4646 - val_mae: 3.3308\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.0164 - mae: 3.2755 - val_loss: 20.4446 - val_mae: 3.3293\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.8507 - mae: 3.3967 - val_loss: 20.1044 - val_mae: 3.2984\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.3906 - mae: 3.4472 - val_loss: 20.2568 - val_mae: 3.3113\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.1630 - mae: 3.4657 - val_loss: 20.3625 - val_mae: 3.3213\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.4203 - mae: 3.2387 - val_loss: 20.0646 - val_mae: 3.2938\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.3287 - mae: 3.4733 - val_loss: 20.1240 - val_mae: 3.2990\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.6581 - mae: 3.3284 - val_loss: 20.1708 - val_mae: 3.3045\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.3139 - mae: 3.3217 - val_loss: 19.9390 - val_mae: 3.2826\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.6120 - mae: 3.4726 - val_loss: 20.1521 - val_mae: 3.3005\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.5817 - mae: 3.3829 - val_loss: 20.1413 - val_mae: 3.2987\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.3222 - mae: 3.3964 - val_loss: 20.0551 - val_mae: 3.2918\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.6489 - mae: 3.4834 - val_loss: 19.9306 - val_mae: 3.2808\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.9730 - mae: 3.4721 - val_loss: 20.2480 - val_mae: 3.3084\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.9119 - mae: 3.2542 - val_loss: 19.9446 - val_mae: 3.2801\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 1s 9ms/step - loss: 48.3093 - mae: 5.0969 - val_loss: 19.6441 - val_mae: 3.2391\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 30.7637 - mae: 4.2072 - val_loss: 23.8186 - val_mae: 3.5806\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.4323 - mae: 3.7497 - val_loss: 24.3110 - val_mae: 3.6250\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.2921 - mae: 3.7270 - val_loss: 24.4117 - val_mae: 3.6348\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.6782 - mae: 3.7614 - val_loss: 25.2689 - val_mae: 3.7105\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 21.2828 - mae: 3.5353 - val_loss: 25.5063 - val_mae: 3.7309\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 21.2388 - mae: 3.5763 - val_loss: 25.3023 - val_mae: 3.7142\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 21.2077 - mae: 3.4843 - val_loss: 25.3011 - val_mae: 3.7139\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.6817 - mae: 3.4765 - val_loss: 24.7876 - val_mae: 3.6671\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 21.0468 - mae: 3.4935 - val_loss: 25.3848 - val_mae: 3.7164\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 21.0020 - mae: 3.5238 - val_loss: 25.2060 - val_mae: 3.7037\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.5460 - mae: 3.5367 - val_loss: 24.6280 - val_mae: 3.6545\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.2480 - mae: 3.4862 - val_loss: 24.5763 - val_mae: 3.6511\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.4093 - mae: 3.3718 - val_loss: 24.3649 - val_mae: 3.6329\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 19.7116 - mae: 3.3824 - val_loss: 24.3188 - val_mae: 3.6295\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 20.4091 - mae: 3.4828 - val_loss: 24.4573 - val_mae: 3.6424\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.4625 - mae: 3.3281 - val_loss: 24.0377 - val_mae: 3.6060\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.8081 - mae: 3.3131 - val_loss: 23.7467 - val_mae: 3.5799\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.9349 - mae: 3.5346 - val_loss: 24.3719 - val_mae: 3.6321\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.0726 - mae: 3.4056 - val_loss: 24.5252 - val_mae: 3.6449\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.2810 - mae: 3.2961 - val_loss: 24.3107 - val_mae: 3.6262\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.5472 - mae: 3.3823 - val_loss: 24.0856 - val_mae: 3.6076\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.9907 - mae: 3.4639 - val_loss: 23.9681 - val_mae: 3.5969\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.6240 - mae: 3.3188 - val_loss: 23.7861 - val_mae: 3.5793\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.1714 - mae: 3.2328 - val_loss: 23.7253 - val_mae: 3.5722\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 17.8920 - mae: 3.2134 - val_loss: 23.4003 - val_mae: 3.5434\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.8428 - mae: 3.3964 - val_loss: 23.2683 - val_mae: 3.5324\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.4658 - mae: 3.3843 - val_loss: 23.5945 - val_mae: 3.5628\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.7483 - mae: 3.4251 - val_loss: 23.7595 - val_mae: 3.5793\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.6558 - mae: 3.3808 - val_loss: 23.5347 - val_mae: 3.5596\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.4840 - mae: 3.2229 - val_loss: 23.3360 - val_mae: 3.5417\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.4601 - mae: 3.2743 - val_loss: 23.0980 - val_mae: 3.5199\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.4335 - mae: 3.4280 - val_loss: 23.2789 - val_mae: 3.5361\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.6414 - mae: 3.3266 - val_loss: 23.5000 - val_mae: 3.5556\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.3242 - mae: 3.3400 - val_loss: 23.0926 - val_mae: 3.5190\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.9105 - mae: 3.4645 - val_loss: 23.1423 - val_mae: 3.5237\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.2204 - mae: 3.3645 - val_loss: 22.9697 - val_mae: 3.5081\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.8617 - mae: 3.2784 - val_loss: 22.7970 - val_mae: 3.4920\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.3992 - mae: 3.3896 - val_loss: 22.8804 - val_mae: 3.5002\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 17.4642 - mae: 3.1480 - val_loss: 22.9055 - val_mae: 3.5032\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.6286 - mae: 3.4176 - val_loss: 22.8279 - val_mae: 3.4952\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.9378 - mae: 3.3744 - val_loss: 22.9175 - val_mae: 3.5033\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.3015 - mae: 3.3744 - val_loss: 22.6748 - val_mae: 3.4804\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 17.8066 - mae: 3.2848 - val_loss: 22.2162 - val_mae: 3.4377\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.9152 - mae: 3.3842 - val_loss: 21.9409 - val_mae: 3.4131\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.6191 - mae: 3.2831 - val_loss: 22.0556 - val_mae: 3.4243\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.2494 - mae: 3.2684 - val_loss: 22.1500 - val_mae: 3.4336\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.4377 - mae: 3.2854 - val_loss: 22.1175 - val_mae: 3.4315\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.9245 - mae: 3.2824 - val_loss: 22.0508 - val_mae: 3.4243\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.9792 - mae: 3.4189 - val_loss: 22.1933 - val_mae: 3.4368\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 1s 8ms/step - loss: 50.9088 - mae: 5.1898 - val_loss: 21.1284 - val_mae: 3.3643\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 24.1068 - mae: 3.7964 - val_loss: 23.0230 - val_mae: 3.5075\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 25.0760 - mae: 3.8423 - val_loss: 23.9266 - val_mae: 3.5784\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.6609 - mae: 3.6328 - val_loss: 23.9158 - val_mae: 3.5754\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.0315 - mae: 3.7135 - val_loss: 24.3187 - val_mae: 3.6053\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 21.2610 - mae: 3.4974 - val_loss: 24.5521 - val_mae: 3.6225\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.1173 - mae: 3.6191 - val_loss: 24.5364 - val_mae: 3.6219\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.6272 - mae: 3.3139 - val_loss: 23.9720 - val_mae: 3.5787\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 21.0156 - mae: 3.4991 - val_loss: 24.1970 - val_mae: 3.5940\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.7893 - mae: 3.5331 - val_loss: 24.2879 - val_mae: 3.6007\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.6336 - mae: 3.4326 - val_loss: 24.5053 - val_mae: 3.6184\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 20.4687 - mae: 3.4817 - val_loss: 24.1537 - val_mae: 3.5925\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.9827 - mae: 3.4089 - val_loss: 24.0470 - val_mae: 3.5854\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.7694 - mae: 3.4599 - val_loss: 23.8611 - val_mae: 3.5709\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.9493 - mae: 3.3244 - val_loss: 23.7359 - val_mae: 3.5614\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.8096 - mae: 3.5428 - val_loss: 23.6819 - val_mae: 3.5583\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.3820 - mae: 3.3770 - val_loss: 23.3641 - val_mae: 3.5354\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.1487 - mae: 3.4590 - val_loss: 23.0036 - val_mae: 3.5075\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.9772 - mae: 3.4464 - val_loss: 22.9125 - val_mae: 3.5000\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.3193 - mae: 3.4819 - val_loss: 22.9870 - val_mae: 3.5059\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.8472 - mae: 3.3742 - val_loss: 22.7818 - val_mae: 3.4900\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.0626 - mae: 3.3118 - val_loss: 22.6455 - val_mae: 3.4795\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.6361 - mae: 3.2417 - val_loss: 22.4422 - val_mae: 3.4633\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 19.4697 - mae: 3.3650 - val_loss: 22.3830 - val_mae: 3.4583\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 20.2292 - mae: 3.4242 - val_loss: 22.3234 - val_mae: 3.4543\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.7045 - mae: 3.2484 - val_loss: 22.4395 - val_mae: 3.4637\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.8137 - mae: 3.2513 - val_loss: 21.9169 - val_mae: 3.4216\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 19.6745 - mae: 3.3586 - val_loss: 22.1397 - val_mae: 3.4408\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.4698 - mae: 3.2812 - val_loss: 22.1445 - val_mae: 3.4408\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 19.4009 - mae: 3.3886 - val_loss: 22.0761 - val_mae: 3.4352\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.4050 - mae: 3.3473 - val_loss: 22.2370 - val_mae: 3.4485\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.6604 - mae: 3.4228 - val_loss: 22.0995 - val_mae: 3.4379\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.4132 - mae: 3.3553 - val_loss: 21.9232 - val_mae: 3.4232\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.9144 - mae: 3.2957 - val_loss: 21.5088 - val_mae: 3.3881\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.4593 - mae: 3.4139 - val_loss: 21.5289 - val_mae: 3.3899\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 17.5694 - mae: 3.1181 - val_loss: 20.7455 - val_mae: 3.3249\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.6762 - mae: 3.4274 - val_loss: 21.0565 - val_mae: 3.3508\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.9416 - mae: 3.2619 - val_loss: 21.5627 - val_mae: 3.3931\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.3296 - mae: 3.2468 - val_loss: 21.0033 - val_mae: 3.3466\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.2292 - mae: 3.2176 - val_loss: 20.4704 - val_mae: 3.3030\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.8799 - mae: 3.3370 - val_loss: 20.5933 - val_mae: 3.3136\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.0996 - mae: 3.3047 - val_loss: 20.8910 - val_mae: 3.3370\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.4210 - mae: 3.1994 - val_loss: 20.8256 - val_mae: 3.3304\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.1410 - mae: 3.3289 - val_loss: 20.5659 - val_mae: 3.3095\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.8687 - mae: 3.2963 - val_loss: 20.5400 - val_mae: 3.3080\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.3330 - mae: 3.2550 - val_loss: 20.5705 - val_mae: 3.3102\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.0118 - mae: 3.3311 - val_loss: 20.7908 - val_mae: 3.3281\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 17.6204 - mae: 3.1777 - val_loss: 20.4453 - val_mae: 3.2994\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.6448 - mae: 3.2485 - val_loss: 20.4069 - val_mae: 3.2975\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 17.8846 - mae: 3.1697 - val_loss: 20.2474 - val_mae: 3.2849\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 1s 8ms/step - loss: 62.9171 - mae: 5.5971 - val_loss: 17.8174 - val_mae: 3.2116\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 37.0519 - mae: 4.5629 - val_loss: 21.9946 - val_mae: 3.6129\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 26.9483 - mae: 3.9133 - val_loss: 23.0440 - val_mae: 3.7212\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 26.2711 - mae: 3.8944 - val_loss: 23.2887 - val_mae: 3.7451\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.9999 - mae: 3.6749 - val_loss: 23.2653 - val_mae: 3.7405\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 24.3630 - mae: 3.6456 - val_loss: 22.9476 - val_mae: 3.7104\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.4797 - mae: 3.6741 - val_loss: 22.9121 - val_mae: 3.7093\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.8235 - mae: 3.6315 - val_loss: 23.3496 - val_mae: 3.7533\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.0178 - mae: 3.5056 - val_loss: 22.9454 - val_mae: 3.7155\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 21.7873 - mae: 3.5227 - val_loss: 22.4215 - val_mae: 3.6645\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 21.3628 - mae: 3.4661 - val_loss: 22.0213 - val_mae: 3.6273\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.8023 - mae: 3.4384 - val_loss: 21.9096 - val_mae: 3.6180\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 21.1151 - mae: 3.4620 - val_loss: 21.7110 - val_mae: 3.5977\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 21.8560 - mae: 3.5126 - val_loss: 21.5945 - val_mae: 3.5859\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 21.8912 - mae: 3.5606 - val_loss: 21.8060 - val_mae: 3.6083\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.1970 - mae: 3.6431 - val_loss: 22.1221 - val_mae: 3.6409\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 21.0681 - mae: 3.4729 - val_loss: 21.8950 - val_mae: 3.6221\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.5066 - mae: 3.4517 - val_loss: 21.6601 - val_mae: 3.5984\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.6789 - mae: 3.3670 - val_loss: 21.3892 - val_mae: 3.5707\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 20.7560 - mae: 3.4332 - val_loss: 21.5015 - val_mae: 3.5821\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 21.2673 - mae: 3.5186 - val_loss: 21.3541 - val_mae: 3.5671\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.3852 - mae: 3.4390 - val_loss: 21.2073 - val_mae: 3.5508\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.9675 - mae: 3.3668 - val_loss: 21.1411 - val_mae: 3.5443\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.4519 - mae: 3.4194 - val_loss: 21.2089 - val_mae: 3.5503\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 18.6677 - mae: 3.2578 - val_loss: 21.0441 - val_mae: 3.5343\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.3706 - mae: 3.3987 - val_loss: 21.3037 - val_mae: 3.5618\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.2764 - mae: 3.3238 - val_loss: 20.9196 - val_mae: 3.5219\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.8129 - mae: 3.3204 - val_loss: 20.6093 - val_mae: 3.4875\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.2732 - mae: 3.3718 - val_loss: 20.4744 - val_mae: 3.4725\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.0573 - mae: 3.5309 - val_loss: 20.8977 - val_mae: 3.5189\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.4914 - mae: 3.4114 - val_loss: 20.8880 - val_mae: 3.5191\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.6873 - mae: 3.3523 - val_loss: 20.5207 - val_mae: 3.4807\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.6469 - mae: 3.3813 - val_loss: 20.4462 - val_mae: 3.4736\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.3010 - mae: 3.3528 - val_loss: 20.3835 - val_mae: 3.4675\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.1845 - mae: 3.2796 - val_loss: 20.3950 - val_mae: 3.4689\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.0922 - mae: 3.3628 - val_loss: 20.4881 - val_mae: 3.4800\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 19.5083 - mae: 3.3494 - val_loss: 20.0953 - val_mae: 3.4392\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.7747 - mae: 3.4111 - val_loss: 20.0704 - val_mae: 3.4358\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.9795 - mae: 3.3627 - val_loss: 20.1500 - val_mae: 3.4424\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.3158 - mae: 3.4353 - val_loss: 19.7216 - val_mae: 3.3967\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.2712 - mae: 3.3552 - val_loss: 19.7899 - val_mae: 3.4050\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.1212 - mae: 3.3371 - val_loss: 19.9068 - val_mae: 3.4176\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.0331 - mae: 3.3330 - val_loss: 19.6834 - val_mae: 3.3953\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.6851 - mae: 3.4431 - val_loss: 19.7497 - val_mae: 3.4018\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.3911 - mae: 3.4099 - val_loss: 19.6150 - val_mae: 3.3882\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.1241 - mae: 3.2552 - val_loss: 19.3090 - val_mae: 3.3570\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.0020 - mae: 3.4221 - val_loss: 19.4540 - val_mae: 3.3726\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.0814 - mae: 3.3616 - val_loss: 19.5631 - val_mae: 3.3827\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.1887 - mae: 3.3914 - val_loss: 19.4068 - val_mae: 3.3656\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 18.6000 - mae: 3.1920 - val_loss: 18.9981 - val_mae: 3.3210\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_models):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_dim=X_train.shape[1], activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "    X_train_bag, _, y_train_bag, _ = train_test_split(X_encoded, y, test_size=0.2, random_state=np.random.randint(1, 1000))\n",
    "    # Train the model on the bootstrap sample\n",
    "    model.fit(X_train_bag, y_train_bag, epochs=50, batch_size=32, validation_split=0.2)  \n",
    "    # Append the trained model to the ensemble\n",
    "    models.append(model)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50d0be6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 619us/step\n",
      "28/28 [==============================] - 0s 700us/step\n",
      "28/28 [==============================] - 0s 651us/step\n",
      "28/28 [==============================] - 0s 660us/step\n",
      "28/28 [==============================] - 0s 629us/step\n",
      "28/28 [==============================] - 0s 609us/step\n",
      "28/28 [==============================] - 0s 667us/step\n"
     ]
    }
   ],
   "source": [
    "all_predictions = []\n",
    "for model in models:\n",
    "    model_predictions = model.predict(X_encoded).squeeze()  # Remove singleton dimensions\n",
    "    all_predictions.append(model_predictions)\n",
    "ensemble_prediction = np.sum(all_predictions, axis=0) / num_models    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd20fd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 19.343689755030663\n",
      "Mean Absolute Error (MAE): 3.242230221322254\n"
     ]
    }
   ],
   "source": [
    "mse = mean_squared_error(y, ensemble_prediction)\n",
    "print(f'Mean Squared Error (MSE): {mse}')\n",
    "\n",
    "mae = mean_absolute_error(y, ensemble_prediction)\n",
    "print(f'Mean Absolute Error (MAE): {mae}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87238b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy within 6.734524543631743 units: 83.30%\n"
     ]
    }
   ],
   "source": [
    "tolerance_multiple = 2\n",
    "tolerance = tolerance_multiple * y.std()\n",
    "# Count the percentage of predictions within the tolerance\n",
    "within_tolerance = np.abs(y - ensemble_prediction) <= tolerance\n",
    "accuracy_percentage = np.mean(within_tolerance) * 100\n",
    "print(f'Accuracy within {tolerance} units: {accuracy_percentage:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee40e6da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
